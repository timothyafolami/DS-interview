{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcccEva1WWrh",
        "outputId": "d40581ab-ffa3-499d-9c8f-ec8aa3a45b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.9/215.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qq sentence_transformers pinecone-client openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqBUxaDw3G0W",
        "outputId": "7dc5eba9-de7c-429a-d511-aaf4091c2cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.8 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m1.6/1.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet  docx2txt langchain_community langchain langchain-pinecone langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "VfpE0oSi3vcu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pinecone import Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from google.colab import userdata\n",
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain_pinecone import PineconeVectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "MZAy8TaKY6pI"
      },
      "outputs": [],
      "source": [
        "#This is for embedding. In here, one LM model from huggingface used.\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gu04tON0cZvT"
      },
      "outputs": [],
      "source": [
        "# #Function to split long documents in to smaller parts\n",
        "# def split_text_into_chunks(plain_text, max_chars=2000):\n",
        "#     text_chunks = []\n",
        "#     current_chunk = \"\"\n",
        "#     for line in plain_text.split(\"\\n\"):\n",
        "#         if len(current_chunk) + len(line) + 1 <= max_chars:\n",
        "#             current_chunk += line + \" \"\n",
        "#         else:\n",
        "#             text_chunks.append(current_chunk.strip())\n",
        "#             current_chunk = line + \" \"\n",
        "#     if current_chunk:\n",
        "#         text_chunks.append(current_chunk.strip())\n",
        "#     return text_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEp6dI0Z7B__"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "MLHqeJrW2ak1"
      },
      "outputs": [],
      "source": [
        "loader = Docx2txtLoader(\"DataLaw.docx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "PXPoACOe2FkC"
      },
      "outputs": [],
      "source": [
        "documents = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "embeddings = OpenAIEmbeddings()\n",
        "index_name = \"lawdata\"\n",
        "docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "8VIZ5_ufbRQ5",
        "outputId": "e5d823f9-02f7-4128-f7a9-291bcd3c6e61"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Legal entities or natural persons that are the owners, lessees or other users of agricultural land (hereinafter: owner, lessee or other user of agricultural land) must allow other persons to carry out, in accordance with regulations, beekeeping, hunting and the recreational gathering of fruits of wild plants, herbaceous wild plants, mushrooms and wild animals on non-arable agricultural land owned or leased by them or otherwise allotted to them and allow other persons free access to non-arable agricultural land provided that no damage is caused by doing so.\\n\\nPersons responsible for damage to land or crops shall be liable for compensation for damage to the owner, lessee or other user of agricultural land or crops in accordance with regulations.\\n\\nArticle 6\\n\\n-         optimally absorbs, retains and releases water. The Government of the Republic of Slovenia (hereinafter: the Government) shall prescribe detailed criteria for permanent soil fertility and plant pollution and mandatory measures related thereto.\\n\\nArticle 4a\\n\\nOn land that is classified as agricultural land according to its designated and actual use, the planting of plantations of miscanthus and plantations of woody, shrub and tree species that are not intended for the cultivation of fruits and olives shall be permitted only when its land rating is less than 30.\\n\\nA plantation under this Article shall mean a compact plantation of plants intended for commercial use with a density of 50 or more plants per hectare.\\n\\nArticle 5'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#This function is responsible for matching the input string with alread existing data on vector database.\n",
        "\n",
        "def find_match(query,k):\n",
        "\n",
        "    result = docsearch.similarity_search(query)\n",
        "    top_k_results = result[:k]  # Get top k results\n",
        "\n",
        "    # Check if results exist (avoid potential errors)\n",
        "    if not top_k_results:\n",
        "        return \"No relevant documents found.\"\n",
        "\n",
        "    # Efficiently merge content using list comprehension\n",
        "    merged_content = \"\\n\\n\".join([d.page_content for d in top_k_results])\n",
        "\n",
        "    return merged_content\n",
        "find_match(\"Hello how are you\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "AoRDzK85aF9E"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def create_prompt(context,query):\n",
        "    #Todo: Should be generated with the context/contexts we find by doing semantaic search\n",
        "    response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo-16k\",\n",
        "    messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a prompt engineer. You take in a context and query. Context is result from the database, while query is the user question. Create a prompt for a LLM model based on both. Context: \" + context + \" Query: \" + query\n",
        "    }\n",
        "],\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    )\n",
        "    return response.choices[0].message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "IyPNrKW3aeoD"
      },
      "outputs": [],
      "source": [
        "def generate_answer(prompt, context):\n",
        "    #Todo: Pass the generated prompt and pass it to gpt-3 to get answers.\n",
        "    response = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo-16k\",\n",
        "      messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"Answer this question correctly based on the context. Context: \" + context\n",
        "      }\n",
        "      ],\n",
        "      temperature=0.7,\n",
        "      max_tokens=256,\n",
        "      top_p=1,\n",
        "      )\n",
        "    return response.choices[0].message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "uWM2IcOKarWz"
      },
      "outputs": [],
      "source": [
        "def user_query(query):\n",
        "  #Todo: Make all the things together.\n",
        "  # Search the database\n",
        "  context = find_match(query,1)\n",
        "  # generate a prompt\n",
        "  prompt = create_prompt(context,query)\n",
        "  # getting the answer:\n",
        "  answer = generate_answer(prompt, context)\n",
        "  # returning the answer\n",
        "  return answer.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "jBds94_gbJ_G",
        "outputId": "1b75cc82-687f-4a71-be17-5b1e4ed3f5ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'If an owner, lessee, or other user of agricultural land does not comply with the requirements stated in paragraph one of this Article, the agricultural inspection authority will impose appropriate measures through a decision. The owner, lessee, or other user must complete these measures within one year, as specified by the decision.'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_query(\"How can I do this?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD9TjzLU96Vj"
      },
      "source": [
        "## Review\n",
        "\n",
        "This is a RAG system.\n",
        "I'm sorry i couldn't use the functions, and that's because I have been using langchain for almost a year and I feel it's way easier to load the data that way.\n",
        "\n",
        "I also used langchain_pinecone library which makes it easy to create the vector database rather than going another way round.\n",
        "\n",
        "My recommendations:\n",
        "\n",
        "- We can make this process more efficient by using langchain modules\n",
        "- Integrating langchain agents will also make the process much more efficient.\n",
        "- The create prompt method can be reviewed to make sure that we are not using too much of credits when getting information from the LLM.\n",
        "- We can also finetune other llms like mistral to make sure we are getting a better response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lljFqr3iwyyO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
